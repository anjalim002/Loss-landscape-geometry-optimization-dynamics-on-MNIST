# -*- coding: utf-8 -*-
"""mnist_geometry.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q177glvXtg3xrOGGbCM79VS0YGvmCf48
"""

"""
Loss Geometry & Optimization Dynamics on MNIST
=============================================

This script trains a small MLP on MNIST using SGD and periodically probes
the loss landscape geometry:

- Top Hessian eigenvalue (sharpness)
- Directional sharpness (loss increase in random directions)
- Loss along a line between two checkpoints (mode connectivity in 1D)


It will:
- train the model
- compute geometry metrics every few epochs
- save the metrics to a JSON file for later analysis
"""

import json
import math
import os
from typing import List, Tuple, Dict

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from torchvision import datasets, transforms


# =========================
# 1. DATA AND MODEL
# =========================

class SimpleMLP(nn.Module):
    """
    Very small MLP for MNIST (flatten -> hidden -> ReLU -> hidden -> ReLU -> logits).
    """

    def __init__(self, hidden_size: int = 256):
        super(SimpleMLP, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28 * 28, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 10)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.flatten(x)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def get_mnist_loaders(batch_size: int = 128) -> Tuple[DataLoader, DataLoader]:
    """
    Download MNIST and create train/test dataloaders.
    """
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

    train_dataset = datasets.MNIST(
        root="./data", train=True, download=True, transform=transform
    )
    test_dataset = datasets.MNIST(
        root="./data", train=False, download=True, transform=transform
    )

    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2
    )
    return train_loader, test_loader


# =========================
# 2. TRAINING UTILITIES
# =========================

def train_one_epoch(model: nn.Module,
                    optimizer: optim.Optimizer,
                    loss_fn,
                    dataloader: DataLoader,
                    device: torch.device) -> float:
    """
    Single training epoch over the full training set.

    Returns average training loss.
    """
    model.train()
    running_loss = 0.0
    count = 0

    for x, y in dataloader:
        x = x.to(device)
        y = y.to(device)

        optimizer.zero_grad()
        logits = model(x)
        loss = loss_fn(logits, y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        count += 1

    return running_loss / max(1, count)


def evaluate_loss_and_accuracy(model: nn.Module,
                               loss_fn,
                               dataloader: DataLoader,
                               device: torch.device) -> Tuple[float, float]:
    """
    Evaluate average loss and accuracy on a dataloader.
    """
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total_examples = 0

    with torch.no_grad():
        for x, y in dataloader:
            x = x.to(device)
            y = y.to(device)
            logits = model(x)
            loss = loss_fn(logits, y)
            total_loss += loss.item() * x.size(0)

            preds = torch.argmax(logits, dim=1)
            total_correct += (preds == y).sum().item()
            total_examples += x.size(0)

    avg_loss = total_loss / max(1, total_examples)
    accuracy = total_correct / max(1, total_examples)
    return avg_loss, accuracy


def flatten_parameters(model: nn.Module) -> torch.Tensor:
    """
    Return all model parameters flattened into a single 1D tensor (on CPU).
    """
    with torch.no_grad():
        return torch.cat([p.detach().cpu().view(-1) for p in model.parameters()])


def set_parameters_from_vector(model: nn.Module, vec: torch.Tensor):
    """
    Overwrite model parameters from a 1D tensor `vec` (on CPU).
    """
    pointer = 0
    for p in model.parameters():
        numel = p.numel()
        new_vals = vec[pointer:pointer + numel].view_as(p)
        with torch.no_grad():
            p.copy_(new_vals.to(p.device))
        pointer += numel


# =========================
# 3. GEOMETRY PROBES
# =========================

def hvp_single_batch(model: nn.Module,
                     loss_fn,
                     dataloader: DataLoader,
                     device: torch.device,
                     v: torch.Tensor) -> torch.Tensor:
    """
    Hessian-vector product H v using autograd on a single batch.

    v is a flattened 1D tensor (on device).
    Returns H v as flattened tensor (on device).
    """
    model.train()  # just to make sure dropout/bn, if any, are active consistently
    params = [p for p in model.parameters() if p.requires_grad]

    # take one batch
    batch_iter = iter(dataloader)
    try:
        x, y = next(batch_iter)
    except StopIteration:
        raise RuntimeError("Dataloader is empty.")

    x = x.to(device)
    y = y.to(device)

    # forward + first gradient
    model.zero_grad()
    logits = model(x)
    loss = loss_fn(logits, y)
    grads = torch.autograd.grad(loss, params, create_graph=True)

    # split v according to params
    v_list = []
    pointer = 0
    for p in params:
        numel = p.numel()
        v_list.append(v[pointer:pointer + numel].view_as(p))
        pointer += numel

    # inner product <grad, v>
    inner = 0.0
    for g, vv in zip(grads, v_list):
        inner = inner + (g * vv).sum()

    # second gradient: grad_w <grad_w L, v> = H v
    hvps = torch.autograd.grad(inner, params, retain_graph=False)

    hvp_flat = torch.cat([h.contiguous().view(-1) for h in hvps]).detach()
    return hvp_flat


def estimate_top_hessian_eig(model: nn.Module,
                             loss_fn,
                             dataloader: DataLoader,
                             device: torch.device,
                             num_power_iters: int = 10) -> float:
    """
    Approximate largest Hessian eigenvalue using power iteration on a single batch.
    """
    theta = flatten_parameters(model).to(device)
    dim = theta.numel()

    # random initial vector
    v = torch.randn(dim, device=device)
    v = v / (v.norm() + 1e-12)

    eigval = 0.0
    for _ in range(num_power_iters):
        hv = hvp_single_batch(model, loss_fn, dataloader, device, v)
        hv = hv.to(device)
        hv_norm = hv.norm()
        if hv_norm.item() == 0.0:
            break
        v = hv / hv_norm
        # Rayleigh quotient (v is normalized)
        eigval = torch.dot(v, hv).item()

    return float(eigval)


def compute_directional_sharpness(model: nn.Module,
                                  loss_fn,
                                  dataloader: DataLoader,
                                  device: torch.device,
                                  epsilon: float = 1e-3,
                                  num_directions: int = 5,
                                  max_batches: int = 1) -> float:
    """
    Approximate directional sharpness by sampling random directions in parameter space.

    For each unit vector u:
        s_epsilon(theta, u) = (L(theta + eps u) - L(theta)) / eps

    Return average over directions.
    """
    original_theta = flatten_parameters(model)
    dim = original_theta.numel()
    base_loss, _ = evaluate_loss_and_accuracy(model, loss_fn, dataloader, device)

    sharp_values: List[float] = []

    for _ in range(num_directions):
        # random direction
        direction = torch.randn(dim)
        direction = direction / (direction.norm() + 1e-12)
        theta_perturbed = original_theta + epsilon * direction

        # set params to perturbed
        set_parameters_from_vector(model, theta_perturbed)

        # loss at perturbed params (on small subset of data)
        model.eval()
        total_loss = 0.0
        total_examples = 0
        with torch.no_grad():
            for batch_idx, (x, y) in enumerate(dataloader):
                x = x.to(device)
                y = y.to(device)
                logits = model(x)
                loss = loss_fn(logits, y)
                total_loss += loss.item() * x.size(0)
                total_examples += x.size(0)
                if batch_idx + 1 >= max_batches:
                    break
        perturbed_loss = total_loss / max(1, total_examples)

        sharp = (perturbed_loss - base_loss) / epsilon
        sharp_values.append(sharp)

        # restore original params before next direction
        set_parameters_from_vector(model, original_theta)

    if len(sharp_values) == 0:
        return 0.0
    return float(sum(sharp_values) / len(sharp_values))


def loss_along_line(model: nn.Module,
                    loss_fn,
                    dataloader: DataLoader,
                    device: torch.device,
                    theta_a: torch.Tensor,
                    theta_b: torch.Tensor,
                    num_points: int = 21,
                    max_batches: int = 5) -> List[Tuple[float, float]]:
    """
    Evaluate loss on a line between theta_a and theta_b.

    Returns list of (alpha, loss), where:
        theta(alpha) = (1-alpha)*theta_a + alpha*theta_b
    """
    alphas = [i / (num_points - 1) for i in range(num_points)]
    results: List[Tuple[float, float]] = []

    backup_theta = flatten_parameters(model)

    for alpha in alphas:
        theta_alpha = (1.0 - alpha) * theta_a + alpha * theta_b
        set_parameters_from_vector(model, theta_alpha)

        # compute loss on small subset of data
        model.eval()
        total_loss = 0.0
        total_examples = 0
        with torch.no_grad():
            for batch_idx, (x, y) in enumerate(dataloader):
                x = x.to(device)
                y = y.to(device)
                logits = model(x)
                loss = loss_fn(logits, y)
                total_loss += loss.item() * x.size(0)
                total_examples += x.size(0)
                if batch_idx + 1 >= max_batches:
                    break

        avg_loss = total_loss / max(1, total_examples)
        results.append((alpha, float(avg_loss)))

    # restore original parameters
    set_parameters_from_vector(model, backup_theta)
    return results


def geometry_difficulty_index(lr: float,
                              lambda_max: float,
                              grad_norm: float,
                              directional_sharpness: float) -> float:
    """
    Compute the Loss Geometry Difficulty Index (LGDI).

    LGDI = (lr * lambda_max * grad_norm) / (1 + directional_sharpness)

    Higher LGDI ~ more unstable, sharper, harder region.
    """
    numerator = lr * lambda_max * grad_norm
    denom = 1.0 + max(0.0, directional_sharpness)
    return float(numerator / denom)


# =========================
# 4. EXPERIMENT DRIVER
# =========================

def run_experiment(
    hidden_size: int = 256,
    lr: float = 0.1,
    epochs: int = 10,
    batch_size: int = 128,
    probe_every: int = 2,
    device: str = "cuda" if torch.cuda.is_available() else "cpu",
    output_dir: str = "./results"
):
    """
    Train MLP on MNIST with SGD, periodically probe loss geometry, and
    save metrics + line loss curve at the end.
    """
    device = torch.device(device)
    os.makedirs(output_dir, exist_ok=True)

    train_loader, test_loader = get_mnist_loaders(batch_size=batch_size)
    model = SimpleMLP(hidden_size=hidden_size).to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)

    # store checkpoints for mode connectivity along the way
    checkpoint_thetas: Dict[int, torch.Tensor] = {}

    # list of per-epoch geometry measurements
    geometry_log: List[Dict[str, float]] = []

    for epoch in range(1, epochs + 1):
        train_loss = train_one_epoch(model, optimizer, loss_fn, train_loader, device)
        test_loss, test_acc = evaluate_loss_and_accuracy(
            model, loss_fn, test_loader, device
        )

        # compute gradient norm (on single batch for speed)
        model.train()
        batch_iter = iter(train_loader)
        x_grad, y_grad = next(batch_iter)
        x_grad = x_grad.to(device)
        y_grad = y_grad.to(device)
        optimizer.zero_grad()
        logits_grad = model(x_grad)
        loss_grad = loss_fn(logits_grad, y_grad)
        loss_grad.backward()
        grad_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                grad_norm += p.grad.detach().norm().item() ** 2
        grad_norm = math.sqrt(grad_norm)

        print(f"Epoch {epoch}/{epochs} "
              f"| train_loss={train_loss:.4f} "
              f"| test_loss={test_loss:.4f} "
              f"| test_acc={100.0 * test_acc:.2f}%")

        # periodically probe geometry
        if epoch % probe_every == 0 or epoch == epochs:
            print("  Probing loss geometry...")
            lambda_max = estimate_top_hessian_eig(
                model, loss_fn, train_loader, device, num_power_iters=5
            )
            dir_sharp = compute_directional_sharpness(
                model, loss_fn, train_loader, device,
                epsilon=1e-3, num_directions=3, max_batches=1
            )
            lgdi = geometry_difficulty_index(
                lr=lr,
                lambda_max=lambda_max,
                grad_norm=grad_norm,
                directional_sharpness=dir_sharp
            )
            stability_ratio = lr * lambda_max  # should be around 2 near edge of stability

            entry = {
                "epoch": float(epoch),
                "train_loss": float(train_loss),
                "test_loss": float(test_loss),
                "test_accuracy": float(test_acc),
                "grad_norm": float(grad_norm),
                "lambda_max": float(lambda_max),
                "directional_sharpness": float(dir_sharp),
                "lgdi": float(lgdi),
                "stability_ratio": float(stability_ratio)
            }
            geometry_log.append(entry)
            checkpoint_thetas[epoch] = flatten_parameters(model)

    # Save geometry log
    metrics_path = os.path.join(output_dir, "mnist_geometry_metrics.json")
    with open(metrics_path, "w") as f:
        json.dump(geometry_log, f, indent=2)
    print(f"Saved geometry metrics to {metrics_path}")

    # Mode connectivity (1D line) between two checkpoints
    if len(checkpoint_thetas) >= 2:
        # use earliest and latest probed epochs
        epochs_sorted = sorted(checkpoint_thetas.keys())
        e1 = epochs_sorted[0]
        e2 = epochs_sorted[-1]
        theta_a = checkpoint_thetas[e1]
        theta_b = checkpoint_thetas[e2]

        print(f"Computing loss along line between epoch {e1} and epoch {e2}...")
        line_losses = loss_along_line(
            model, loss_fn, test_loader, device,
            theta_a, theta_b, num_points=21, max_batches=5
        )

        line_path = os.path.join(output_dir, "line_loss.json")
        with open(line_path, "w") as f:
            json.dump(
                [{"alpha": float(a), "loss": float(l)} for (a, l) in line_losses],
                f,
                indent=2
            )
        print(f"Saved line loss curve to {line_path}")

    print("Experiment finished.")


if __name__ == "__main__":
    # You can tweak these hyperparameters if you want.
    run_experiment(
        hidden_size=256,
        lr=0.1,
        epochs=10,
        batch_size=128,
        probe_every=2,
        device="cuda" if torch.cuda.is_available() else "cpu",
        output_dir="./results_mnist"
    )